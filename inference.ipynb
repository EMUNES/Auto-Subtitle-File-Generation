{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('asg': conda)"
  },
  "interpreter": {
   "hash": "7c1e26b787ea84c0136f54dc33c70b3c054a8e0cf94179623b14c46360f5a1e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Predict the timestamps for possible subtitles of your input audio**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from catalyst.dl import SupervisedRunner, CallbackOrder, Callback, CheckpointCallback\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_type, extract_audio, mono_load, get_duration\n",
    "from config import InferenceConfig as IC\n",
    "from csrc.configurations import DatasetConfig as DC\n",
    "from csrc.configurations import ModelConfig as MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better debugging\n",
    "\n",
    "os.environ[\"CUDA_LAUCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "source": [
    "## User configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT PARAM\n",
    "PERIOD = 5 # 10, 5, 3, 2, 1\n",
    "### IMPORTANT PARAM\n",
    "THRESHOLD = 0.8 # 0.5, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those need no modification.\n",
    "\n",
    "# Audio Sample rate. DO NOT change.\n",
    "SR = DC.dataset_sample_rate\n",
    "\n",
    "# Tag encoding. DO NOT change.\n",
    "CODING = IC.coding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "!Warning: Output file already exists, are you sure to rewrite the file?\nUSING MODEL: ./train/logs/sp2-32000hz/checkpoints/best.pth\n"
     ]
    }
   ],
   "source": [
    "# Set and audiohandler your input file.\n",
    "\n",
    "### Target file for inferencing.\n",
    "TARGET_FILE_PATH = \"./src/src-test/src-test.wav\"\n",
    "\n",
    "TARGET_FILE_PATH = extract_audio(TARGET_FILE_PATH) if check_type(TARGET_FILE_PATH) else TARGET_FILE_PATH\n",
    "\n",
    "### Output csv file name.\n",
    "OUTPUT_FILE_NAME = \"test\"\n",
    "\n",
    "OUTPUT_FILE = f\"./inf/{OUTPUT_FILE_NAME}.csv\"\n",
    "OUTPUT_SOURCE_FILE = f\"./inf/{OUTPUT_FILE_NAME}-all.csv\"\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(\"!Warning: Output file already exists, are you sure to rewrite the file?\")\n",
    "\n",
    "# Target model used for this prediction. Must be corresponding to the model during the training process.\n",
    "\n",
    "### Set the model file path. The file path must be valid\n",
    "MODEL_PATH = \"./train/logs/sp2-32000hz/checkpoints/best.pth\"\n",
    "\n",
    "print(f\"USING MODEL: {MODEL_PATH}\")"
   ]
  },
  {
   "source": [
    "## Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrc.dataset import PANNsDataset"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrc.models import PANNsCNN14Att, AttBlock"
   ]
  },
  {
   "source": [
    "## Inference Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PANNsCNN14Att(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_block1): ConvBlock(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block2): ConvBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block3): ConvBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block4): ConvBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block5): ConvBlock(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block6): ConvBlock(\n",
       "    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (att_block): AttBlock(\n",
       "    (att): Conv1d(2048, 2, kernel_size=(1,), stride=(1,))\n",
       "    (cla): Conv1d(2048, 2, kernel_size=(1,), stride=(1,))\n",
       "    (bn_att): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "model = PANNsCNN14Att(**MC.sed_model_config)\n",
    "model.att_block = AttBlock(2048, 2, activation='sigmoid')\n",
    "model.att_block.init_weights()\n",
    "model.load_state_dict(torch.load(MODEL_PATH)['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading file: ./src/src-test/src-test.wav\n"
     ]
    }
   ],
   "source": [
    "y, _ = mono_load(TARGET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()"
   ]
  },
  {
   "source": [
    "## Prediction begin!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There is inconsistency between the audio waveform and header metadata.This could be ignored.\nAudio file duration: 7462.54934375s\n"
     ]
    }
   ],
   "source": [
    "from utils import get_duration\n",
    "audio_duration = get_duration(audio_file_path=TARGET_FILE_PATH, y=y, sr=SR)\n",
    "\n",
    "print(f\"Audio file duration: {audio_duration}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='1493' class='' max='1493' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [1493/1493 00:19<00:00]\n    </div>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "audios = []\n",
    "\n",
    "len_y = len(y)\n",
    "start = 0\n",
    "end = PERIOD * SR\n",
    "\n",
    "# Split audio into clips.\n",
    "while True:\n",
    "    y_batch = y[start:end].astype(np.float32)\n",
    "    if len(y_batch) != PERIOD * SR:\n",
    "        y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n",
    "        y_pad[:len(y_batch)] = y_batch\n",
    "        audios.append(y_pad)\n",
    "        break\n",
    "    start = end\n",
    "    end += PERIOD * SR\n",
    "    audios.append(y_batch)\n",
    "\n",
    "# Get tensors\n",
    "arrays = np.asarray(audios)\n",
    "tensors = torch.from_numpy(arrays)\n",
    "\n",
    "estimated_event_list = []\n",
    "global_time = 0.0\n",
    "for image in progress_bar(tensors):\n",
    "    image = image.view(1, image.size(0))\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image)\n",
    "        framewise_outputs = prediction[\"framewise_output\"].detach(\n",
    "            ).cpu().numpy()[0]\n",
    "            \n",
    "    thresholded = framewise_outputs >= THRESHOLD\n",
    "\n",
    "    for target_idx in range(thresholded.shape[1]):\n",
    "        if thresholded[:, target_idx].mean() == 0:\n",
    "            pass\n",
    "        else:\n",
    "            detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "            head_idx = 0\n",
    "            tail_idx = 0\n",
    "            while True:\n",
    "                if (tail_idx + 1 == len(detected)) or (\n",
    "                        detected[tail_idx + 1] - \n",
    "                        detected[tail_idx] != 1):\n",
    "                    onset = 0.01 * detected[\n",
    "                        head_idx] + global_time\n",
    "                    offset = 0.01 * detected[\n",
    "                        tail_idx] + global_time\n",
    "                    onset_idx = detected[head_idx]\n",
    "                    offset_idx = detected[tail_idx]\n",
    "                    max_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].max()\n",
    "                    mean_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].mean()\n",
    "                    estimated_event = {\n",
    "                        \"speech_recognition\": CODING[target_idx],\n",
    "                        \"start\": onset,\n",
    "                        \"end\": offset,\n",
    "                        \"max_confidence\": max_confidence,\n",
    "                        \"mean_confidence\": mean_confidence,\n",
    "                    }\n",
    "                    estimated_event_list.append(estimated_event)\n",
    "                    head_idx = tail_idx + 1\n",
    "                    tail_idx = tail_idx + 1\n",
    "                    if head_idx >= len(detected):\n",
    "                        break\n",
    "                else:\n",
    "                    tail_idx += 1\n",
    "    global_time += PERIOD\n",
    "    \n",
    "prediction_df = pd.DataFrame(estimated_event_list)"
   ]
  },
  {
   "source": [
    "## Post process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure output file offset: max offset should be less than audio duration.\n",
    "\n",
    "max_offset = prediction_df.iloc[-1].end\n",
    "if max_offset > audio_duration:\n",
    "    prediction_df.iloc[-1].end = audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.speech_recognition==\"speech\"].to_csv(OUTPUT_FILE, index=False)\n",
    "prediction_df.to_csv(OUTPUT_SOURCE_FILE, index=False)"
   ]
  }
 ]
}